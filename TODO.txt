To do:
    Presentatie opbouwen vanaf het begin
        Verhaal van afgelopen meeting er in verwerken
        Zodra af, de plots er in zetten

    Scoring van Clayton - sJoe KL tuned werkt niet LogS geeft dist die bell-shaped rondom 11 zit en CS & CLS bell-shape rondom 5???
        Same data is used for tuning & scoring --> sJoe is over-fitted?

        DM stat is soms zero variance -> zorgt dit voor verkeerde rej_freq? ; komt dit door te klein score verschil door afronden?

        RuntimeWarning: divide by zero encountered in scalar divide (1.0 - p_R) * np.log((1.0 - p_R) / (1.0 - f_R))
            Dit gebeurt als de Fw_bar niet goed (pre)calculated is en 1 of 0 is

        Lijkt er op dat het misschien aan de indicator formule ligt --> X
            je kan niet als je op de marginals sum je quantile zet deze ook gebruiken voor de pits sum
            dat zou alleen zo zijn als de marginals uniform zijn en PIT transform een linear transform is
            Goede manier zou zijn:
                generate PITs, convert naar marginals, compute sum & find q_0.05 (kan analytically)
                kijk naar de PIT samples die voor X + Y < q_0.05 zorgen, geef deze PIT sample pairs weight 1 en anders 0

        Fw_bar precomp from every model using large sample in Ramon, in mijn code Fw_bar per model ook, maar w is daarna recomputed met u_{t+1}
            Dit geeft Fw is heel smooth, maar w is extreem noisy omdat het maar 1 punt gebruikt

        Bij Ramon lijkt het of elke DM de full time-series length T is used, in mijn code DM uses P windows, but each score only using one obs u_{t+1}
            Hier is dan ook waarschijnlijk mijn code heel noisy

    Bivariate bonferroni bounds?

    Hellinger, chi-squared, ... divergences instead of KL

To do morgen:
    Fix het CS, CLS deel van de simulation die nu kapot lijkt te zijn

Boundary uncertainty extension:
    Maak de restriction transformation met een andere copula dan de f of g voor size en power test credibility (Mitchell & Wealle example)
    Kijk naar andere weight function dan I

Simulation study:
    Repeat de 2010 onderzoek van diks et al, maar gebruik nu de true u'tjes ipv ecdf based als 'echte' gebruiken (size en power vergelijken)
        --> score differences tussen de symmetricaly misspecified f en g zijn distributed rond 1 ipv 0
        --> kijk wat diks et al study doet met de forecasted u en de rolling window -> rolling misschien niet nodig bij mijn versie???
	    --> Kijk hoe fitten van de andere copulas effect heeft op empirical??? (true is bb7 en nu gebruiken we ook bb7 om de boundary te bekijken)

Scriptie meeting 15-5:
    Bij schatten van de u_hats eerste keer worden ze in 2025 paper daarna gebruikt als de echte u'tjes. Als je die u'tjes al zet van tevoren weet je wat de echte zijn. Je kan dan de size en power plots maken met de echte utjes en met de u_hats gebruikt als echte u'tjes. Dan kijken wat het verschil is op de size en power plots.
    Je moet de boundry transformation niet laten afhangen van de copula die je kiest voor je f of g omdat je anders met twee maten meet (als a wisselt tussen f en p en tussen g en p). De boundary moet door een derde copula transformed worden.
        Er is dan dus eigenlijk geen true & waar kan je de bonferroni bounds op zetten?
    Vraag ramon naar hoe hij de numerical integral doet
    Hangt het q'tje nu ook niet af van de f of g? --> maakt waarschijnlijk niet uit

Afgelopen meeting:
    PLOT CDF
    Compare first innovations (ARMA-GARCH vb (V)AR(1) SCOMDY) vs residuals effect on the ECDF vs ORACLE plots ()
    Lastly introduce boundary uncertainty for (innov +( ECDF or ORACLE )) or (reside + (ECDF or oracle))
    Robustness checks at end of paper
    Write on the effect of correctly choosing which model to estimate (vb true = AR1, but estimated is AR2) -> also ECDF vs inverse of AR(1) if you assume AR(1) is true
    Slides must be on combinations of uncertainties in the models
    In slides show emperical u vs oracle u (show multiple estimated u lines)

Scriptie algemeen:
BLIJF SCHRIJVEN IN OVERLEAF
    1. Maak methodologie robuust -> #DONE# begin bij transcriben van de meeting
    2. Maak de collection van papers om te citeren groter (zotero), heb al een aantal gevonden met chat, vind image recog paper
    3. Stop notes in markdown in de repository voor future reference
    4. Bekijk volledige code van Ramon en importeer functies om te gebruiken
    5. Gebruik alle info om de methodologie te maken in code



Notes/vragen:
    Als ik tGARCH(1,1) marginals wil met bb7_copula dependence als true moet ik inverse ecdf gebruiken op de innovations van een simulated tGARCH(1,1)
        Dan komt er al uncertainty door de ecdf bij kijken.
        Alleen als ik een analytical inverse CDF kan gebruiken zoals voor gewoon student t innovations dan heb ik geen uncertainty
            Gebruik inverse ECDF van mega sample om quantile function te maken (mogelijk deze smoothen)
        Is het wel nodig om tGARCH(1,1) marginals te gebruiken in deze simulation study, want de copula vraagt om standardized residuals?
        Als we de residuals van de tGARCH standardizen dan krijg je gewoon student-t distributed marginals die je gebruikt???
        GARCH deel heeft te maken met time dependence, niet cross sectional wat volgens mij alleen zou worden bekeken in deze simulation setup.
        Overschakelen op student-t marginals en analytical inverse CDF?

    Wat zijn normale waardes die de copula density aanneemt wanneer het likely is dat er joint dependence is?
        --> Lijkt me dat 1 independence is en dan 0-1 unlikely en 1+ likely, kan infinite worden

    Wat is het numerically integraten en waarom is dit belangrijk?
        Stel grid is op [0,1]^2 en 300x300 dan heeft elke cell oppervlakte (1/300)^2. Moet deze constant ook keer de uiteindelijke sum (integral approximation)
        Waarom was dit zo een issue volgens ramon?

    Moet ik als ik de ECDF based regions maak en bootstrap/simulation doe de y1 en y2 gebruiken die uit true marginals + true copula komen en dan daar de copula of interest op fitten en dan een nieuwe y series maken?
        Of moet ik gewoon y1 en y2 uit de student-t halen zoals de true en dan gelijk een copula of interest daar op toepassen om daarvan y series met de dependence van die copula te krijgen en dan op een manier vergelijken met de true dependence copula?
        Moet ik de copula of interest fitten? of kan ik gewoon parameters zetten? fitten lijkt me handiger richting empirisch nut

    Moet ik uberhaupt kijken naar smoothing van ECDF door quantile regression of kernel interpolation. Of is dat alleen handig als we een percentile willen zoeken?


