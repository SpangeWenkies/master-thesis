To do:

    Code ramon (2025 paper) gebruikt geen copula's
        Wel een multivariate part die te gebruiken is

    KLIC matching doen
        Laat clayton de rotated Gumbel worden

    Fix het tweede deel van de simulation die nu kapot lijkt te zijn



To do morgen:


Boundary uncertainty extension:
    Maak de restriction transformation met een andere copula dan de f of g voor size en power test credibility (Mitchell & Wealle example)
    Kijk naar andere weight function dan I

Simulation study:
    Repeat de 2010 onderzoek van diks et al, maar gebruik nu de true u'tjes ipv ecdf based als 'echte' gebruiken (size en power vergelijken)
        --> score differences tussen de symmetricaly misspecified f en g zijn distributed rond 1 ipv 0
        --> kijk wat diks et al study doet met de forecasted u en de rolling window
	    --> Kijk hoe fitten van de andere copulas effect heeft op empirical??? (true is bb7 en nu gebruiken we ook bb7 om de boundary te bekijken)

Scriptie meeting 15-5:
    Bij schatten van de u_hats eerste keer worden ze in 2025 paper daarna gebruikt als de echte u'tjes. Als je die u'tjes al zet van tevoren weet je wat de echte zijn. Je kan dan de size en power plots maken met de echte utjes en met de u_hats gebruikt als echte u'tjes. Dan kijken wat het verschil is op de size en power plots.
    Je moet de boundry transformation niet laten afhangen van de copula die je kiest voor je f of g omdat je anders met twee maten meet (als a wisselt tussen f en p en tussen g en p). De boundary moet door een derde copula transformed worden.
        Er is dan dus eigenlijk geen true & waar kan je de bonferroni bounds op zetten?
    Vraag ramon naar hoe hij de numerical integral doet
    Hangt het q'tje nu ook niet af van de f of g? --> maakt waarschijnlijk niet uit

Code algemeen:
	Als too many configs maak dan een class of een namespace
	#DONE# CS EN CLS ZIJN FOUT

Scriptie algemeen:
BLIJF SCHRIJVEN IN OVERLEAF
    1. Maak methodologie robuust -> #DONE# begin bij transcriben van de meeting
    2. Maak de collection van papers om te citeren groter (zotero), heb al een aantal gevonden met chat, vind image recog paper
    3. Stop notes in markdown in de repository voor future reference
    4. Bekijk volledige code van Ramon en importeer functies om te gebruiken
    5. Gebruik alle info om de methodologie te maken in code



Notes/vragen:
    Als ik tGARCH(1,1) marginals wil met bb7_copula dependence als true moet ik inverse ecdf gebruiken op de innovations van een simulated tGARCH(1,1)
        Dan komt er al uncertainty door de ecdf bij kijken.
        Alleen als ik een analytical inverse CDF kan gebruiken zoals voor gewoon student t innovations dan heb ik geen uncertainty
            Gebruik inverse ECDF van mega sample om quantile function te maken (mogelijk deze smoothen)
        Is het wel nodig om tGARCH(1,1) marginals te gebruiken in deze simulation study, want de copula vraagt om standardized residuals?
        Als we de residuals van de tGARCH standardizen dan krijg je gewoon student-t distributed marginals die je gebruikt???
        GARCH deel heeft te maken met time dependence, niet cross sectional wat volgens mij alleen zou worden bekeken in deze simulation setup.
        Overschakelen op student-t marginals en analytical inverse CDF?

    Wat zijn normale waardes die de copula density aanneemt wanneer het likely is dat er joint dependence is?
        --> Lijkt me dat 1 independence is en dan 0-1 unlikely en 1+ likely, kan infinite worden

    Wat is het numerically integraten en waarom is dit belangrijk?
        Stel grid is op [0,1]^2 en 300x300 dan heeft elke cell oppervlakte (1/300)^2. Moet deze constant ook keer de uiteindelijke sum (integral approximation)
        Waarom was dit zo een issue volgens ramon?

    Moet ik als ik de ECDF based regions maak en bootstrap/simulation doe de y1 en y2 gebruiken die uit true marginals + true copula komen en dan daar de copula of interest op fitten en dan een nieuwe y series maken?
        Of moet ik gewoon y1 en y2 uit de student-t halen zoals de true en dan gelijk een copula of interest daar op toepassen om daarvan y series met de dependence van die copula te krijgen en dan op een manier vergelijken met de true dependence copula?
        Moet ik de copula of interest fitten? of kan ik gewoon parameters zetten? fitten lijkt me handiger richting empirisch nut

    Moet ik uberhaupt kijken naar smoothing van ECDF door quantile regression of kernel interpolation. Of is dat alleen handig als we een percentile willen zoeken?

